# Informe de Contribuci칩n Individual - Reto Connect-4
**Universidad de La Sabana | Facultad de Ingenier칤a**
**Asignatura:** Fundamentos de Inteligencia Artificial
**Estudiante:** Giovanni Moreno
**Fecha:** 24 de Noviembre de 2025

---

## 1. Resumen de Contribuci칩n
Mi rol principal en el proyecto fue el dise침o y la implementaci칩n de la arquitectura h칤brida del agente (`GioImprovedPolicy`). Ante las limitaciones de tiempo de c칩mputo y la necesidad de un rendimiento robusto desde la primera partida, decid칤 descartar un enfoque puro de Monte Carlo Tree Search (MCTS) en favor de una soluci칩n que integra **B칰squeda Adversaria (Minimax)** con **Aprendizaje por Refuerzo Tabular**.

Me enfoqu칠 en dotar al agente de una "intuici칩n base" mediante heur칤sticas posicionales para evitar el problema de arranque en fr칤o (Cold Start) t칤pico de los agentes de RL, asegurando as칤 el cumplimiento del requisito de vencer al jugador aleatorio desde la semana 1.

## 2. Aportes Destacados (Highlights)

A continuaci칩n, presento mis dos contribuciones t칠cnicas m치s significativas:

### Aporte A: Implementaci칩n del Motor H칤brido (Minimax + Q-Learning)
Dise침칠 la l칩gica central en `act` y `minimax` para que el agente tome decisiones bas치ndose en una b칰squeda de profundidad limitada (`depth=4`) con Poda Alfa-Beta. La innovaci칩n clave fue integrar la **Q-Table** como funci칩n de evaluaci칩n en los nodos hoja: si el estado es conocido, el agente usa su "memoria" (RL); si es desconocido, utiliza la heur칤stica. Esto permite validaci칩n emp칤rica del aprendizaje sin sacrificar competencia inmediata.

* **Archivo:** `groups/C/policy.py`
* **Evidencia (Commit):** [游댕 Ver Commit: Integraci칩n final H칤brida Q-Learning + Minimax](ENLACE_A_TU_COMMIT_DEL_DIA_23_AQUI)

### Aporte B: Dise침o de Heur칤stica Posicional y Optimizaci칩n de Poda
Para optimizar la velocidad del algoritmo Minimax, implement칠 una heur칤stica matem치tica (`score_position`) que valora el control del centro y las ventanas de 4 fichas. Adicionalmente, implement칠 una l칩gica de ordenamiento de acciones (`sorted(valid_cols, key=lambda x: abs(x - 3))`) que prioriza la evaluaci칩n de columnas centrales. Esto maximiza la ocurrencia de podas Alfa-Beta, reduciendo dr치sticamente el tiempo de c칩mputo y permitiendo una b칰squeda m치s profunda.

* **Archivo:** `groups/C/policy.py`
* **Evidencia (Commit):** [游댕 Ver Commit: Heur칤stica posicional y Center Control](ENLACE_A_TU_COMMIT_DEL_DIA_21_AQUI)

---

## 3. Desaf칤os y Logros

**Desaf칤os Enfrentados:**
* **Latencia en Python:** La implementaci칩n inicial de Minimax era lenta. El desaf칤o fue optimizar la poda. Descubr칤 que al evaluar primero las jugadas centrales (estad칤sticamente mejores), el algoritmo descarta ramas in칰tiles mucho m치s r치pido.
* **Gesti칩n de Memoria:** Un MCTS puro reseteaba el conocimiento en cada turno. El reto fue estructurar la persistencia de datos para que el agente pudiera "recordar" entre partidas (usando `pickle`), integrando esto dentro de la estructura de carpetas del torneo (`groups/C`).

**Principales Logros:**
* **Invencibilidad vs. Random:** Se logr칩 un agente que gana el 100% de las partidas contra un jugador aleatorio, cumpliendo el requisito cr칤tico de la primera entrega.
* **Defensa Reactiva:** Implementaci칩n de un sistema de "bloqueo de emergencia" que detecta victorias inminentes del rival antes de entrar a la b칰squeda profunda, evitando derrotas por "ceguera de horizonte".

---

## 4. Reflexi칩n y Propuestas de Mejora

**Reflexi칩n de la Soluci칩n:**
La soluci칩n h칤brida demostr칩 ser superior a las heur칤sticas est치ticas y al MCTS con pocas simulaciones. La capacidad de usar una Q-Table permite que el agente evolucione, mientras que la heur칤stica garantiza que nunca juegue de forma "tonta" en estados nuevos. Sin embargo, la dependencia de una tabla hash limita la generalizaci칩n: el agente no entiende que dos estados son similares, solo sabe si son id칠nticos.

**Propuestas de Mejora:**
1.  **Uso de Bitboards:** Reemplazar la matriz de Numpy por operaciones a nivel de bits (bitwise operations). Esto permitir칤a evaluar el tablero en nanosegundos y aumentar la profundidad de b칰squeda de 4 a 8 o m치s.
2.  **Aproximaci칩n de Funciones (Deep RL):** Sustituir la Q-Table tabular por una peque침a Red Neuronal. Esto permitir칤a al agente generalizar patrones visuales y estimar el valor de estados nunca vistos, reduciendo el uso de memoria RAM.